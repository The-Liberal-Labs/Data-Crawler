# Data-Insighter Architecture Documentation

Complete architecture documentation for the Data-Insighter AI-powered data analysis platform.

## Table of Contents

1. [System Overview](#system-overview)
2. [High-Level Architecture](#high-level-architecture)
3. [Why DuckDB?](#why-duckdb)
4. [Query Processing Flow](#query-processing-flow)
5. [LangGraph Workflow](#langgraph-workflow)
6. [Agent Breakdown](#agent-breakdown)
7. [Verification Pipeline](#verification-pipeline)
8. [System Capabilities](#system-capabilities)
9. [Directory Structure](#directory-structure)

## System Overview

**Data-Insighter** is an AI-powered data analysis platform that converts natural language queries into executable Python/SQL code for analyzing CSV/tabular data. It uses a sophisticated multi-agent LangGraph workflow with a 4-layer verification pipeline to ensure analytical accuracy on **entire datasets** (not samples).

### Primary Use Case

Market research survey data analysis requiring accurate analytics on the full dataset.

### Tech Stack

| Component   | Technology                       |
|-------------|----------------------------------|
| Frontend    | Streamlit (Python)               |
| Backend     | FastAPI (Python 3.10+)           |
| Workflow    | LangGraph                        |
| AI Engine   | Google Gemini (flash, pro, lite) |
| User Data   | PostgreSQL                       |
| Analytics   | DuckDB (in-process)              |

## High-Level Architecture

```
flowchart TB
    subgraph Frontend["Frontend Layer"]
        UI[Streamlit UI<br/>app.py]
    end

    subgraph Backend["Backend Layer"]
        API[FastAPI Backend<br/>api/main.py]

        subgraph Routers["API Routers"]
            AUTH[auth.py]
            ANALYSIS[analysis.py]
            DATASETS[datasets.py]
            CHAT[chat.py]
        end

        subgraph Services["Services"]
            SVC[analysis_service.py]
        end

        subgraph Graph["LangGraph Workflow"]
            GEN[generator.py<br/>33 Nodes]
            EXEC[executor.py<br/>3 Execution Modes]
            NODES[nodes/<br/>36 Processing Nodes]
        end
    end

    subgraph External["External Services"]
        GEMINI[Google Gemini<br/>LLM API]
    end

    subgraph DataLayer["Data Layer"]
        PG[(PostgreSQL<br/>Users, Sessions, Chats)]
        DUCK[(DuckDB<br/>Analytics Engine)]
        FILES[(CSV/Parquet<br/>Data Files)]
    end

    UI -->|HTTP| API
    API --> AUTH & ANALYSIS & DATASETS & CHAT
    ANALYSIS --> SVC
    SVC --> GEN
    GEN --> EXEC
    EXEC --> NODES
    NODES -->|LLM Calls| GEMINI
    NODES -->|User Data| PG
    NODES -->|Analytics| DUCK
    DUCK -->|Query| FILES
```

### Component Responsibilities

| Component          | Purpose                                                                             |
|--------------------|-------------------------------------------------------------------------------------|
| Streamlit UI       | User interface for file uploads, natural language queries, and result visualization |
| FastAPI Backend    | REST API handling authentication, dataset management, and analysis orchestration    |
| LangGraph Workflow | Multi-agent orchestration with conditional routing and state management             |
| PostgreSQL         | Transactional data: users, sessions, chats, dataset metadata, query logs            |
| DuckDB             | High-performance analytics engine for full-dataset SQL queries                      |
| Google Gemini      | LLM for natural language understanding, code generation, and validation             |

## Why DuckDB?

### The Problem with Traditional Approaches

Traditional data analysis approaches load entire datasets into pandas memory, which creates several issues:

1. **Memory Limits** : Large datasets (&gt;1GB) can exhaust system memory
2. **Performance** : pandas operations on large datasets are slow
3. **Sampling Bias** : Sampling data for LLM context can lead to inaccurate analysis
4. **Scalability** : Cannot handle production-scale data volumes

### The DuckDB-First Solution

Data-Insighter uses a **DuckDB-first architecture** where:

```
flowchart LR
    subgraph Traditional["Traditional Approach"]
        CSV1[CSV File] -->|Load ALL| DF1[pandas DataFrame<br/>in Memory]
        DF1 -->|Full Data| LLM1[LLM Context]
        DF1 -->|Process| RESULT1[Results]
    end

    subgraph DuckDB["DuckDB-First Approach"]
        CSV2[CSV File] -->|10 rows only| SAMPLE[Sample for<br/>LLM Context]
        CSV2 -->|Direct Query| DUCK2[(DuckDB<br/>In-Process)]
        SAMPLE -->|Schema + Examples| LLM2[LLM]
        LLM2 -->|Generate SQL| DUCK2
        DUCK2 -->|Query ALL rows| RESULT2[Results]
    end
```

### Where DuckDB is Used

`data_loader_node.py` `simple_query_handler_node.py` `Python_code_executer_node.py` `numerical_claim_verifier_node.py` `data_profiler_node.py`

| Location     | File   | Purpose   |
|--------------|--------|-----------|
| Data Loading |        |           |

| Extract schema, row count, sample rows   |
|------------------------------------------|

| Fast SQL for basic aggregations   |
|-----------------------------------|

| Analytics object injected into sandbox   |
|------------------------------------------|

| Independent SQL recalculation   |
|---------------------------------|

| Statistical analysis via SQL   |
|--------------------------------|

### DuckDB Benefits

| Benefit          | Description                             |
|------------------|-----------------------------------------|
| No Memory Limits | Queries run on-disk, not in-memory      |
| 10-20x Faster    | Columnar storage + vectorized execution |
| Full Dataset     | All N rows analyzed, not samples        |
| Thread Safe      | Ephemeral connections per query         |
| Zero Config      | In-process, no external server          |

### DuckDB Configuration

```
DUCKDB_MEMORY_LIMIT=8GB    # Max memory per query
DUCKDB_THREADS=8           # Parallel threads
```

## Query Processing Flow

### Complete User Query Flow

```
flowchart TB
    subgraph Input["User Input"]
        USER[User Query:<br/>"What is the average age<br/>by region?"]
        DATA[Dataset:<br/>customers.csv]
    end

    subgraph Phase1["Phase 1: Initialization"]
        LOAD[load_data<br/>DuckDB schema extraction]
        RELEVANCY[check_query_relevancy<br/>Is query related to data?]
        REWRITE[re_write_query<br/>Clarify ambiguous queries]
        CACHE[check_cache<br/>Semantic similarity check]
    end

    subgraph Phase2["Phase 2: Routing"]
        SIMPLE[check_simple_query<br/>Pattern detection]
        ROUTE[route_query<br/>Multi-agent routing]
    end

    subgraph Phase3["Phase 3: Analysis"]
        PARALLEL[parallel_agents<br/>Profiler + Selector + Quality]
        DECOMPOSE[decompose_query<br/>Break into subtasks]
        CODEGEN[generate_python_code<br/>DuckDB SQL + Python]
        EXECUTE[execute_python_code<br/>Secure sandbox]
    end

    subgraph Phase4["Phase 4: Verification"]
        VERIFY[numerical_verifier<br/>Claim verification]
        VALIDATE[validate_output<br/>Semantic validation]
        CRITIQUE[self_critique<br/>Hallucination detection]
        CONFIDENCE[calculate_confidence<br/>Final scoring]
    end

    subgraph Output["Output"]
        REPORT[generate_report<br/>Markdown formatting]
        RESULT[Analysis Report<br/>+ Confidence Score]
    end

    USER & DATA --> LOAD
    LOAD -->|Success| RELEVANCY
    LOAD -->|Fail| ERROR1[Error Report]
    RELEVANCY -->|Relevant| REWRITE
    RELEVANCY -->|Irrelevant| ERROR2[Relevancy Report]
    REWRITE --> CACHE
    CACHE -->|Hit| VALIDATE
    CACHE -->|Miss| SIMPLE
    SIMPLE -->|Simple| EXEC_SIMPLE[Execute SQL]
    SIMPLE -->|Complex| ROUTE
    ROUTE -->|Conversational| CHATBOT[chatbot]
    ROUTE -->|Data Analysis| PARALLEL
    PARALLEL --> DECOMPOSE
    DECOMPOSE --> CODEGEN
    CODEGEN --> EXECUTE
    EXECUTE -->|Success| VERIFY
    EXECUTE -->|Fail| REGEN[Regenerate Code]
    REGEN --> CODEGEN
    VERIFY -->|Verified| VALIDATE
    VERIFY -->|Discrepancies| REGEN
    VALIDATE -->|Pass| REPORT
    VALIDATE -->|Fail| REGEN
    REPORT --> CRITIQUE
    CRITIQUE -->|Approved| CONFIDENCE
    CRITIQUE -->|Needs Revision| REVISE[Revise Report]
    REVISE --> CRITIQUE
    CONFIDENCE --> RESULT
    EXEC_SIMPLE --> RESULT
    CHATBOT --> RESULT
```

### Query Decomposition Example

For complex queries, the system breaks them into subtasks:

```
flowchart TB
    QUERY["Complex Query:<br/>'Compare sales trends by region<br/>and identify top products'"]

    DECOMPOSE[decompose_query Node]

    subgraph SubTasks["Generated Subtasks"]
        T1["Subtask 1:<br/>Calculate sales by region"]
        T2["Subtask 2:<br/>Analyze temporal trends"]
        T3["Subtask 3:<br/>Rank products by sales"]
        T4["Subtask 4:<br/>Cross-tabulate results"]
    end

    COMBINE[Combine Results]
    OUTPUT[Unified Report]

    QUERY --> DECOMPOSE
    DECOMPOSE --> T1 & T2 & T3 & T4
    T1 & T2 & T3 & T4 --> COMBINE
    COMBINE --> OUTPUT
```

## LangGraph Workflow

### Workflow Definition

The workflow is defined in `api/graph/generator.py` with **33 nodes** and conditional routing edges.

### Complete Workflow Graph

```
flowchart TB
    START((START))

    subgraph Init["Initialization"]
        load_data["load_data"]
        check_query_relevancy["check_query_relevancy"]
        re_write_query["re_write_query"]
        check_cache["check_cache"]
    end

    subgraph Routing["Routing Layer"]
        check_simple_query["check_simple_query"]
        execute_simple_query["execute_simple_query"]
        route_query["route_query"]
        chatbot["chatbot"]
    end

    subgraph Parallel["Parallel Agents"]
        parallel_agents["parallel_agents"]
        data_profiler["data_profiler"]
        column_selector["column_selector"]
        data_quality["data_quality"]
    end

    subgraph CodeGen["Code Generation"]
        decompose_query["decompose_query"]
        generate_python_code["generate_python_code"]
        multi_task_code_generator["multi_task_code_generator"]
        Python_code_sanitize["sanitize_code"]
        execute_python_code["execute_python_code"]
        re_generate_python_code["re_generate_python_code"]
    end

    subgraph Verify["Verification Pipeline"]
        numerical_verifier["numerical_verifier"]
        validate_output["validate_output"]
        validate_output_lite["validate_output_lite"]
        save_to_cache["save_to_cache"]
    end

    subgraph Report["Reporting"]
        generate_report["generate_report"]
        self_critique["self_critique"]
        revise_report["revise_report"]
        calculate_confidence["calculate_confidence"]
    end

    subgraph Fallback["Error Handling"]
        fallback_analysis["fallback_analysis"]
        data_load_error["data_load_error_report"]
        query_relevancy_report["query_relevancy_report"]
    end

    END((END))

    START --> load_data
    load_data -->|Success| check_query_relevancy
    load_data -->|Fail| data_load_error --> END

    check_query_relevancy -->|Relevant| re_write_query
    check_query_relevancy -->|Irrelevant| query_relevancy_report --> END

    re_write_query --> check_cache
    check_cache -->|Hit| validate_output
    check_cache -->|Miss| check_simple_query

    check_simple_query -->|Simple| execute_simple_query
    check_simple_query -->|Complex| route_query
    execute_simple_query -->|Done| END
    execute_simple_query -->|Complex| route_query

    route_query -->|Conversational| chatbot
    route_query -->|Data Analysis| parallel_agents

    chatbot -->|Pure Chat| generate_report
    chatbot -->|Needs Code| parallel_agents

    parallel_agents --> data_profiler & column_selector & data_quality
    data_profiler & column_selector & data_quality --> decompose_query

    decompose_query -->|Single| generate_python_code
    decompose_query -->|Multi| multi_task_code_generator

    generate_python_code --> Python_code_sanitize
    multi_task_code_generator --> Python_code_sanitize

    Python_code_sanitize -->|Safe| execute_python_code
    Python_code_sanitize -->|Unsafe| re_generate_python_code

    execute_python_code -->|Success| numerical_verifier
    execute_python_code -->|Fail| re_generate_python_code
    re_generate_python_code --> generate_python_code

    numerical_verifier -->|Verified| validate_output
    numerical_verifier -->|Discrepancies| re_generate_python_code

    validate_output -->|Pass| save_to_cache
    validate_output -->|Fail| fallback_analysis
    validate_output_lite --> save_to_cache

    save_to_cache --> generate_report
    fallback_analysis --> generate_report

    generate_report --> self_critique

    self_critique -->|Approved| calculate_confidence
    self_critique -->|Needs Revision| revise_report
    self_critique -->|Rejected| re_generate_python_code

    revise_report --> self_critique

    calculate_confidence --> END
```

### Node Categories

| Category        | Nodes                                                         | Purpose                              |
|-----------------|---------------------------------------------------------------|--------------------------------------|
| Initialization  | load_data, check_query_relevancy, re_write_query, check_cache | Data loading and query preprocessing |
| Routing         | check_simple_query, route_query, chatbot                      | Query classification and routing     |
| Parallel Agents | parallel_agents, data_profiler, column_selector, data_quality | Concurrent data analysis             |
| Code Generation | decompose_query, generate_python_code, sanitize, execute      | Code creation and execution          |
| Verification    | numerical_verifier, validate_output, self_critique            | 4-layer accuracy verification        |
| Reporting       | generate_report, revise_report, calculate_confidence          | Output formatting and scoring        |
| Error Handling  | fallback_analysis, error_report nodes                         | Graceful degradation                 |

## Agent Breakdown

### Multi-Agent System Architecture

```
flowchart LR
    subgraph Orchestrator["LangGraph Orchestrator"]
        STATE[AgentState<br/>Shared Context]
    end

    subgraph Specialists["Specialist Agents"]
        PROFILER["Data Profiler<br/>Statistics & distributions"]
        SELECTOR["Column Selector<br/>Relevant column mapping"]
        QUALITY["Data Quality<br/>Missing values, outliers"]
        ROUTER["Query Router<br/>Analysis type classification"]
        DECOMPOSER["Query Decomposer<br/>Subtask breakdown"]
        CODEGEN["Code Generator<br/>DuckDB SQL + Python"]
        VERIFIER["Numerical Verifier<br/>Claim verification"]
        CRITIC["Self-Critic<br/>Hallucination detection"]
    end

    STATE <--> PROFILER & SELECTOR & QUALITY & ROUTER & DECOMPOSER & CODEGEN & VERIFIER & CRITIC
```

### Detailed Agent Descriptions

#### 1. Data Profiler Agent ( `data_profiler_node.py` )

**Purpose** : Comprehensive data structure analysis

**Capabilities** :

- Column type detection (numeric, categorical, datetime, text)
- Statistical summaries (mean, median, std, min, max, quartiles)
- Distribution analysis (normal, skewed, bimodal)
- Cardinality analysis (unique values, distinct counts)
- Missing value patterns
- Outlier detection (IQR, z-score)
- Correlation matrix for numeric columns

**Output** : `unified_profile` in AgentState

#### 2. Column Selector Agent ( `column_selector_node.py` )

**Purpose** : Map user query terms to relevant columns

**Capabilities** :

- Fuzzy matching for typos ("revnue" → "revenue")
- Semantic column mapping ("sales" → "total\_amount")
- Primary vs secondary column classification
- Domain-specific vocabulary handling

**Output** : `selected_columns` , `primary_columns` in AgentState

#### 3. Data Quality Agent ( `data_quality_node.py` )

**Purpose** : Assess data quality and flag issues

**Capabilities** :

- Missing value analysis by column
- Duplicate row detection
- Type consistency checking
- Outlier identification
- Data completeness scoring
- Quality recommendations

**Output** : `data_quality_score` , `data_quality_issues` in AgentState

#### 4. Query Router Agent ( `query_router_node.py` )

**Purpose** : Classify query type and select analysis approach

**Analysis Types Detected** :

| Type             | Description                | Example                       |
|------------------|----------------------------|-------------------------------|
| STATISTICAL      | Aggregations, correlations | "What is the average age?"    |
| VISUALIZATION    | Charts and plots           | "Show sales trend"            |
| COMPARISON       | A/B tests, cohort analysis | "Compare regions"             |
| EXPLORATION      | Data overview              | "Summarize the data"          |
| SPECIFIC_LOOKUP  | Filter and find            | "Find customers over 50"      |
| TREND_ANALYSIS   | Time series                | "Sales growth over time"      |
| CROSS_TABULATION | Pivot tables               | "Sales by region and product" |
| PREDICTIVE       | Forecasting                | "Predict next quarter"        |
| CONVERSATIONAL   | Greetings, help            | "Hello"                       |

**Output** : `query_type` , `analysis_types` , `requires_visualization` in AgentState

#### 5. Query Decomposer Agent ( `decompose_query_node.py` )

**Purpose** : Break complex queries into executable subtasks

**Capabilities** :

- Multi-part query detection
- Dependency ordering
- Subtask prioritization
- Result combination strategy

**Output** : `sub_tasks` , `total_sub_tasks` in AgentState

#### 6. Code Generator Agent ( `Python_code_generator_node.py` )

**Purpose** : Generate DuckDB-first Python code

**Key Principles** :

- Uses DuckDB SQL for ALL aggregations
- Queries run on ENTIRE dataset (not samples)
- Plotly for visualizations
- Follows secure execution patterns

**Code Template** :

```
# Pre-injected: analytics, data_file, pd, np, px, go

# Analytics via DuckDB (runs on ALL rows)
result = analytics.execute_sql_to_dict(data_file, """
    SELECT region, AVG(age) as avg_age
    FROM data
    GROUP BY region
""")
print(f"Average age by region: {result}")

# Visualization
df = pd.DataFrame(result)
fig = px.bar(df, x='region', y='avg_age')
fig.write_html('visualizations/chart.html')
print("[INTERACTIVE_CHART:visualizations/chart.html]")
```

**Output** : `Python_Code` in AgentState

#### 7. Numerical Verifier Agent ( `numerical_claim_verifier_node.py` )

**Purpose** : Independently verify numerical claims

**Process** :

1. Extract claims via regex ("average is 45.2")
2. Identify relevant columns
3. Recalculate via DuckDB SQL
4. Compare claimed vs actual (tolerance: 10%)
5. Flag discrepancies

**Sanity Checks** :

- Percentages: 0-100
- Correlations: -1 to 1
- Counts: non-negative
- P-values: 0-1

**Output** : `numerical_verification_status` , `numerical_discrepancies` in AgentState

#### 8. Self-Critic Agent ( `self_critique_node.py` )

**Purpose** : Detect hallucinations and alignment issues

**Verifications** :

1. **Claim Verification** : All factual claims backed by data
2. **Query Alignment** : Report answers the question asked
3. **Scope Creep** : No unrequested analyses
4. **Statistical Language** : Appropriate hedging and caveats

**Verdicts** :

- `APPROVED` : Ready for output
- `NEEDS_REVISION` : Minor fixes needed (max 2 iterations)
- `REJECTED` : Regenerate from scratch

**Output** : `critique_verdict` , `hallucination_count` , `claims_verified` in AgentState

## Verification Pipeline

### 4-Layer Verification Architecture

```
flowchart TB
    EXEC[Code Execution<br/>Results]

    subgraph Layer1["Layer 1: Numerical Verification"]
        EXTRACT[Extract Claims<br/>via Regex]
        RECALC[Recalculate<br/>via DuckDB SQL]
        COMPARE[Compare Values<br/>Tolerance: 10%]
    end

    subgraph Layer2["Layer 2: Output Validation"]
        SEMANTIC[LLM Semantic<br/>Validation]
        STRUCTURE[Structure &<br/>Format Check]
        COMPLETE[Completeness<br/>Assessment]
    end

    subgraph Layer3["Layer 3: Report Generation"]
        FORMAT[Markdown<br/>Formatting]
        INSIGHTS[Key Insights<br/>Extraction]
        RECOMMEND[Recommendations<br/>Generation]
    end

    subgraph Layer4["Layer 4: Self-Critique"]
        HALLUCINATE[Hallucination<br/>Detection]
        ALIGN[Query<br/>Alignment]
        SCOPE[Scope Creep<br/>Detection]
    end

    CONFIDENCE[Confidence<br/>Scoring]
    OUTPUT[Final Report<br/>+ Score]

    EXEC --> Layer1
    Layer1 --> Layer2
    Layer2 --> Layer3
    Layer3 --> Layer4
    Layer4 --> CONFIDENCE
    CONFIDENCE --> OUTPUT

    EXTRACT --> RECALC --> COMPARE
    SEMANTIC --> STRUCTURE --> COMPLETE
    FORMAT --> INSIGHTS --> RECOMMEND
    HALLUCINATE --> ALIGN --> SCOPE
```

### Verification Details

| Layer   | Purpose                  | Method                          | Failure Action         |
|---------|--------------------------|---------------------------------|------------------------|
| Layer 1 | Numerical accuracy       | Deterministic SQL recalculation | Regenerate code        |
| Layer 2 | Output quality           | LLM semantic validation         | Regenerate or fallback |
| Layer 3 | Report formatting        | Template-based generation       | N/A (always succeeds)  |
| Layer 4 | Hallucination prevention | LLM claim verification          | Revise or regenerate   |

### Confidence Scoring

The final confidence score (0.0-1.0) aggregates signals from all layers:

`very_high` `high` `moderate` `low` `very_low`

| Score Range   | Level   | Interpretation   |
|---------------|---------|------------------|
| 0.90-1.00     |         |                  |

| Results are highly reliable   |
|-------------------------------|

| Results are reliable with minor caveats   |
|-------------------------------------------|

| Results need additional verification   |
|----------------------------------------|

| Results may be inaccurate   |
|-----------------------------|

| Results should not be trusted   |
|---------------------------------|

## System Capabilities

### What Data-Insighter Can Do

#### Statistical Analysis

- Descriptive statistics (mean, median, mode, std, variance)
- Correlation analysis (Pearson, Spearman)
- Regression analysis (linear, multiple)
- Hypothesis testing (t-tests, chi-square, ANOVA)
- Distribution analysis

#### Data Exploration

- Automatic data profiling
- Missing value analysis
- Outlier detection
- Data quality assessment
- Column type inference

#### Visualization

- Bar charts, line charts, scatter plots
- Histograms, box plots, violin plots
- Heatmaps, correlation matrices
- Pie charts, treemaps
- Interactive Plotly charts

#### Query Types Supported

- Aggregations ("What is the average...")
- Comparisons ("Compare X vs Y")
- Trends ("How has X changed over time")
- Rankings ("Top 10 by...")
- Filters ("Find all where...")
- Cross-tabulations ("X by Y and Z")
- Distributions ("Distribution of...")

#### Natural Language Understanding

- Ambiguous query clarification
- Typo tolerance
- Domain vocabulary mapping
- Multi-part query decomposition
- Conversational context

### Performance Characteristics

| Metric                     | Value                |
|----------------------------|----------------------|
| Max dataset size           | Limited only by disk |
| Query timeout              | 30 seconds           |
| Max retries                | 5 (configurable)     |
| Cache TTL                  | 24 hours             |
| Cache similarity threshold | 0.75                 |

### Security Features

- JWT authentication with refresh tokens
- Secure code sandbox (AST validation, restricted builtins)
- Module allowlist (pandas, numpy, plotly only)
- Per-user dataset isolation
- No eval/exec/import allowed

## Directory Structure

```
Data-Insighter/
├── api/                          # FastAPI Backend
│   ├── core/                     # Configuration & utilities
│   │   ├── config.py            # Pydantic settings
│   │   ├── logging.py           # Structured logging
│   │   └── exceptions.py        # Custom exceptions
│   ├── database/                # Data layer
│   │   ├── pg_connection.py     # PostgreSQL pool
│   │   ├── pg_repositories.py   # Data access layer
│   │   ├── duckdb_analytics.py  # DuckDB engine
│   │   └── models.py            # ORM models
│   ├── routers/                 # HTTP Endpoints
│   │   ├── auth.py              # Authentication
│   │   ├── analysis.py          # Query execution
│   │   ├── datasets.py          # File management
│   │   └── chat.py              # Chat sessions
│   ├── services/                # Business logic
│   │   └── analysis_service.py  # Graph executor wrapper
│   └── main.py                  # FastAPI entry point
├── api/graph/                   # LangGraph Workflow
│   ├── generator.py             # Graph builder (33 nodes)
│   ├── executor.py              # Execution modes
│   ├── nodes/                   # 36 Processing nodes
│   │   ├── agent_state.py       # State definition
│   │   ├── data_loader_node.py
│   │   ├── query_router_node.py
│   │   ├── Python_code_generator_node.py
│   │   ├── numerical_claim_verifier_node.py
│   │   ├── self_critique_node.py
│   │   └── [30+ more nodes]
│   ├── prompts/                 # LLM prompts
│   └── utils/                   # 23 utilities
│       ├── sandbox.py           # Secure execution
│       ├── semantic_cache.py    # Query caching
│       └── [20+ more utils]
├── client/                      # Python API client
├── docs/                        # Documentation
├── app.py                       # Streamlit Frontend
├── docker-compose.yml           # PostgreSQL Docker
└── requirements.txt             # Dependencies
```

## AgentState Reference

The `AgentState` TypedDict (defined in `api/graph/nodes/agent_state.py` ) is the shared context passed between all nodes.

### Key State Fields

```
# Core Query
query: str                    # Original user question
rephrased_query: str          # Clarified query
csv_file_path: str            # Path to data file

# DuckDB Architecture
data_file_path: str           # Path for DuckDB queries
parquet_path: str             # Converted Parquet path
actual_row_count: int         # TRUE row count (not sample)
data_schema: dict             # {column: DuckDB_type}
sample_rows_for_context: list # 10 rows for LLM

# Routing
query_type: str               # STATISTICAL, VISUALIZATION, etc.
analysis_types: list          # Applicable analysis types
requires_visualization: bool

# Verification
numerical_verification_status: str  # VERIFIED, PARTIAL, DISCREPANCIES
numerical_claims_verified: int
numerical_discrepancies: list
validation_status: str             # PASS, NEEDS_IMPROVEMENT, FAIL

# Self-Critique
critique_verdict: str              # APPROVED, NEEDS_REVISION, REJECTED
hallucination_count: int
claims_verified: int

# Confidence
analysis_confidence_score: float   # 0.0-1.0
analysis_confidence_level: str     # very_high, high, moderate, low, very_low

# Output
reports: str                       # Final markdown report
execution_results: str             # Raw execution output
```

## Execution Modes

The workflow can be executed in three modes via `api/graph/executor.py` :

### 1. Synchronous Execution

```
report = get_reports(
    user_query="What is the average age?",
    csv_file_path="/path/to/data.csv",
    column_description="Age: customer age in years"
)
```

### 2. With Feedback Tracking

```
result = await get_reports_with_feedback(...)
# Returns ExecutionResult with detailed progress
```

### 3. Streaming (SSE)

```
async for event in get_reports_streaming(...):
    # StreamEvent with real-time progress
    # event_type: NODE_START, NODE_END, PROGRESS, COMPLETE
```

## Quick Reference

### Starting the System

```
# 1. Start PostgreSQL
docker-compose up -d postgres

# 2. Start API (port 8000)
uvicorn api.main:app --reload

# 3. Start Streamlit UI (port 8501)
streamlit run app.py
```

### API Endpoints

`/api/v1/auth/login` `/api/v1/datasets/upload` `/api/v1/analysis/query` `/api/v1/analysis/query/stream` `/api/v1/chat/sessions`

| Endpoint   | Method   | Purpose   |
|------------|----------|-----------|
|            |          |           |

| POST   | Authenticate user   |
|--------|---------------------|

| POST   | Upload CSV file   |
|--------|-------------------|

| POST   | Run analysis query   |
|--------|----------------------|

| POST   | SSE streaming   |
|--------|-----------------|

| GET/POST   | Manage chat sessions   |
|------------|------------------------|

### Environment Variables

```
GOOGLE_API_KEY=xxx              # Required
GEMINI_MODEL=gemini-3-flash     # LLM model
POSTGRES_PORT=5433              # PostgreSQL port
DUCKDB_MEMORY_LIMIT=8GB         # Analytics memory
MAX_RETRIES=5                   # Code regeneration limit
```

*Last updated: January 2026*